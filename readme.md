
---

# RAG Chatbot using LangChain, FAISS, and Gemini API

## ğŸ“Œ Overview

This project builds a **Retrieval-Augmented Generation (RAG) Chatbot** using:

* **LangChain** for constructing the retrieval + generation pipeline
* **FAISS** as the vector store for similarity search
* **Google Gemini API** for generating natural and accurate responses

The chatbot can answer questions based on **your own documents** through a retrieval process combined with language model generation.

---

## ğŸš€ Key Features

* ğŸ” **Semantic search** powered by FAISS
* ğŸ“„ **Supports multiple document types**: PDF, text, docx
* âœ‚ï¸ **Automatic chunking + embedding generation**
* ğŸ§  **Full RAG pipeline**: Retriever â†’ LLM â†’ Answer
* ğŸ’¬ **Context-aware conversation support** (history-aware retriever)
* âš¡ **Fast deployment** with FastAPI
* ğŸ“¦ **Local FAISS storage** to reduce costs

---

## Demo

![Demo](./demo.gif)

## ğŸ“‚ Project Structure

```
project/
â”œâ”€â”€ docs                     # Document storage
â”œâ”€â”€ db                       # vector storage
src/
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â””â”€â”€ RAG.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ragas.py
â”‚
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ lms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test.ipynb
â”‚   â””â”€â”€ test.py
â”‚
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Bm25Retriever.py
â”‚   â”œâ”€â”€ FaissRetriever.py
â”‚   â””â”€â”€ HybridRetriever.py
â”‚
â”œâ”€â”€ splitters/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ TextSplitter.py
â”‚
â”œâ”€â”€ stores/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Bm25Store.py
â”‚   â””â”€â”€ FaissStore.py
â”‚
â””â”€â”€ UI/
â”‚    â””â”€â”€ index.html         
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env                     # API keys
```

## ğŸ› ï¸ Technologies Used

* Python 3.13
* LangChain 1.x
* FAISS
* Google Gemini API
* FastAPI
* uvicorn
* HuggingFace

---

## ğŸ”§ Installation

### 1ï¸âƒ£ Clone the project

```
git clone https://github.com/8thMay03/RAG-system.git
cd RAG-system
```

### 2ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Create the `.env` file

```env
GOOGLE_API_KEY=your_api_key_here
```

## â–¶ï¸ Run the FastAPI server from RAG-system folder

```
python -m src.app
```

The API runs at:

```
http://127.0.0.1:8000/
```

## â–¶ï¸ Run the UI

Open the `index.html` file.

---

## ğŸ”— How the RAG pipeline works

1. The user sends a question
2. The system generates an embedding from the query
3. FAISS retrieves the most relevant document chunks
4. LangChain combines the context with the query
5. Gemini API generates an answer based on the documents

---

## ğŸ“œ License

MIT License.

---
